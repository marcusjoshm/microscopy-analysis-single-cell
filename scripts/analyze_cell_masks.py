#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Cell Mask Analysis Script

This script analyzes cell masks generated from segmentation to extract features
and measurements. It processes the watershed masks and CSV files generated by 
ImageJ/Fiji macro to create a standardized dataset of cell features.
"""

import os
import sys
import argparse
import pandas as pd
import numpy as np
import logging
import glob
from pathlib import Path
import re
from datetime import datetime
import tifffile
import skimage.measure
import skimage.io
from scipy import ndimage

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("cell_analysis.log"),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger("CellAnalysis")

def extract_metadata_from_filename(filename):
    """
    Extract metadata (region, timepoint, etc.) from filename.
    
    Args:
        filename (str): Filename to extract metadata from
        
    Returns:
        dict: Dictionary containing extracted metadata
    """
    metadata = {}
    
    # Extract region (e.g., R_1)
    region_match = re.search(r'(R_\d+)', filename)
    if region_match:
        metadata['region'] = region_match.group(1)
    
    # Extract timepoint (e.g., t00)
    timepoint_match = re.search(r'(t\d+)', filename)
    if timepoint_match:
        metadata['timepoint'] = timepoint_match.group(1)
    
    # Extract condition from parent directory
    path = Path(filename)
    parent_dir = path.parent.name
    if parent_dir.startswith("Dish_"):
        metadata['condition'] = parent_dir
    
    return metadata

def process_cell_measurements(measurements_file, output_dir, add_features=True):
    """
    Process cell measurements CSV file from ImageJ and add derived features.
    
    Args:
        measurements_file (str): Path to CSV file with cell measurements
        output_dir (str): Directory to save processed results
        add_features (bool): Whether to compute additional features
        
    Returns:
        pd.DataFrame: Processed cell measurements
    """
    logger.info(f"Processing measurements file: {measurements_file}")
    
    # Read the measurements CSV
    try:
        df = pd.read_csv(measurements_file)
        logger.info(f"Found {len(df)} cells in {measurements_file}")
    except Exception as e:
        logger.error(f"Error reading measurements file: {e}")
        return None
    
    # Extract metadata from filename
    metadata = extract_metadata_from_filename(measurements_file)
    
    # Add metadata columns
    for key, value in metadata.items():
        df[key] = value
    
    # Add a unique cell ID
    df['cell_id'] = [f"{metadata.get('condition', 'unknown')}_{metadata.get('region', 'unknown')}_{metadata.get('timepoint', 'unknown')}_cell{i}" 
                     for i in range(len(df))]
    
    # Calculate derived features if requested
    if add_features:
        # Shape features
        df['circularity'] = (4 * np.pi * df['Area']) / (df['Perim.'] ** 2)
        df['aspect_ratio'] = df['Major'] / df['Minor']
        df['roundness'] = 4 * df['Area'] / (np.pi * df['Major'] ** 2)
        df['solidity'] = df['Area'] / df['Area']  # Placeholder - would need convex hull area
        
        # Intensity features - normalize
        if 'Mean' in df.columns:
            df['normalized_intensity'] = df['Mean'] / df['Mean'].max()
        
        # Positional features - normalize to image size
        if 'X' in df.columns and 'Y' in df.columns:
            # We would need image dimensions for proper normalization
            # This is a placeholder
            df['relative_x'] = df['X'] / df['X'].max()
            df['relative_y'] = df['Y'] / df['Y'].max()
            
            # Distance from center (assuming center is at max/2)
            center_x = df['X'].max() / 2
            center_y = df['Y'].max() / 2
            df['dist_from_center'] = np.sqrt((df['X'] - center_x)**2 + (df['Y'] - center_y)**2)
    
    # Save the processed measurements
    output_file = os.path.join(output_dir, os.path.basename(measurements_file).replace(".csv", "_processed.csv"))
    df.to_csv(output_file, index=False)
    logger.info(f"Saved processed measurements to {output_file}")
    
    return df

def analyze_watershed_mask(mask_file, measurements_df, output_dir):
    """
    Analyze watershed mask to extract additional features not captured by ImageJ.
    
    Args:
        mask_file (str): Path to watershed mask TIFF file
        measurements_df (pd.DataFrame): DataFrame with cell measurements
        output_dir (str): Directory to save processed results
        
    Returns:
        pd.DataFrame: Updated cell measurements with additional features
    """
    logger.info(f"Analyzing watershed mask: {mask_file}")
    
    try:
        # Read the mask image
        mask = tifffile.imread(mask_file)
        
        # Make sure it's a labeled mask (each cell has a unique integer ID)
        if mask.max() == 1:  # Binary mask
            logger.info("Converting binary mask to labeled mask")
            mask = skimage.measure.label(mask)
        
        # Get region properties
        props = skimage.measure.regionprops(mask)
        logger.info(f"Found {len(props)} regions in mask")
        
        # Extract additional properties
        additional_features = []
        for i, prop in enumerate(props):
            # Skip small regions (likely noise)
            if prop.area < 10:
                continue
                
            features = {
                'label': prop.label,
                'eccentricity': prop.eccentricity,
                'equivalent_diameter': prop.equivalent_diameter,
                'euler_number': prop.euler_number,
                'extent': prop.extent,
                'orientation': prop.orientation,
                'perimeter': prop.perimeter,
                'solidity': prop.solidity
            }
            additional_features.append(features)
        
        # Convert to DataFrame
        mask_df = pd.DataFrame(additional_features)
        
        if len(mask_df) == 0:
            logger.warning("No valid regions found in mask")
            return measurements_df
        
        # Try to match cells in measurements_df with regions in mask_df
        # This is a simplistic approach - in practice, you'd need a more robust matching method
        if len(mask_df) == len(measurements_df):
            logger.info("Same number of cells in measurements and mask - direct mapping")
            for col in mask_df.columns:
                if col != 'label':
                    measurements_df[f'mask_{col}'] = mask_df[col].values
        else:
            logger.warning(f"Mismatch between measurements ({len(measurements_df)} cells) and mask ({len(mask_df)} regions)")
            # Could implement more sophisticated matching here
            
        # Save the updated measurements
        output_file = os.path.join(output_dir, os.path.basename(mask_file).replace(".tif", "_enhanced.csv"))
        measurements_df.to_csv(output_file, index=False)
        logger.info(f"Saved enhanced measurements to {output_file}")
        
        return measurements_df
        
    except Exception as e:
        logger.error(f"Error analyzing watershed mask: {e}")
        return measurements_df

def main():
    """Main entry point for the script."""
    parser = argparse.ArgumentParser(description='Cell Mask Analysis Script')
    
    parser.add_argument('--input', '-i', required=True,
                        help='Input directory containing mask measurement files')
    parser.add_argument('--output', '-o', required=True,
                        help='Output directory for processed results')
    parser.add_argument('--regions', '-r', nargs='+', default=[],
                        help='Specific regions to analyze (e.g., R_1 R_2 R_3)')
    parser.add_argument('--timepoints', '-t', nargs='+', default=[],
                        help='Specific timepoints to analyze (e.g., t00 t03)')
    parser.add_argument('--skip-enhanced', '-s', action='store_true',
                        help='Skip enhanced feature extraction from masks')
    
    args = parser.parse_args()
    
    # Convert regions and timepoints to sets for faster lookup
    regions = set(args.regions)
    timepoints = set(args.timepoints)
    
    # Create output directory if it doesn't exist
    os.makedirs(args.output, exist_ok=True)
    
    # Find all measurement CSV files
    measurement_files = []
    for root, dirs, files in os.walk(args.input):
        for file in files:
            if file.endswith("_measurements.csv"):
                measurement_files.append(os.path.join(root, file))
    
    logger.info(f"Found {len(measurement_files)} measurement files")
    
    # Process each measurement file
    all_cells = []
    for meas_file in measurement_files:
        # Extract metadata
        metadata = extract_metadata_from_filename(meas_file)
        
        # Check if this file matches specified regions/timepoints
        if regions and metadata.get('region') not in regions:
            logger.info(f"Skipping {meas_file} - region not in specified list")
            continue
            
        if timepoints and metadata.get('timepoint') not in timepoints:
            logger.info(f"Skipping {meas_file} - timepoint not in specified list")
            continue
        
        # Process measurements
        cell_df = process_cell_measurements(meas_file, args.output)
        
        if cell_df is not None and not args.skip_enhanced:
            # Look for corresponding watershed mask
            mask_file = meas_file.replace("_measurements.csv", "_watershed.tif")
            if os.path.exists(mask_file):
                # Analyze mask for enhanced features
                cell_df = analyze_watershed_mask(mask_file, cell_df, args.output)
            else:
                logger.warning(f"No watershed mask found for {meas_file}")
        
        if cell_df is not None:
            all_cells.append(cell_df)
    
    # Combine all cell data if any was processed
    if all_cells:
        combined_df = pd.concat(all_cells, ignore_index=True)
        logger.info(f"Combined data for {len(combined_df)} cells from {len(all_cells)} files")
        
        # Save combined dataset
        combined_output = os.path.join(args.output, "all_cells_combined.csv")
        combined_df.to_csv(combined_output, index=False)
        logger.info(f"Saved combined cell data to {combined_output}")
    else:
        logger.warning("No cell data was processed")
    
    logger.info("Cell analysis complete")


if __name__ == "__main__":
    main() 